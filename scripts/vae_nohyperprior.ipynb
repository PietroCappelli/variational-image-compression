{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder with no hyperprior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 22:45:15.699003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 22:45:15.837590: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-04 22:45:16.458822: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ubuntu/data/miniconda3/envs/tf2/lib/\n",
      "2023-03-04 22:45:16.458932: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ubuntu/data/miniconda3/envs/tf2/lib/\n",
      "2023-03-04 22:45:16.458940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_compression as tfc\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from BalleFFP import BalleFFP\n",
    "from read_data import read_data_numpy\n",
    "\n",
    "import constants\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_settings() -> tf.distribute.MirroredStrategy:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1']) # gpu distribution strategy\n",
    "    return strategy\n",
    "\n",
    "def read_data(ch_format='channels_first') -> np.ndarray:\n",
    "    data_path = os.path.join(constants.DATA_FOLDER, constants.DATA_FILE)\n",
    "    data      = read_data_numpy(data_path, ch_format)\n",
    "    return data\n",
    "\n",
    "def split_train_test(data: np.ndarray):\n",
    "    train_images = data[:constants.TRAINING_SET_SIZE]\n",
    "    test_images  = data[constants.TRAINING_SET_SIZE:constants.TRAINING_SET_SIZE+constants.VALIDATION_SET_SIZE]\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 22:45:22.236614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.238003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.246561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.247808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.249025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.250201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.253071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 22:45:22.619990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.621445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.622762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.623994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.625203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:22.626396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.555299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.556761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.558076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.559515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.560721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.561945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13641 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5\n",
      "2023-03-04 22:45:23.562514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 22:45:23.563748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13641 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n",
      "Reading data...\n",
      "Data shape: (100000, 3, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "strategy  = gpu_settings()\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "print('Reading data...')\n",
    "ch_format = 'channels_first'\n",
    "data      = read_data(ch_format)\n",
    "print('Data shape: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (80000, 3, 96, 96)\n",
      "Distributing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 22:45:58.119445: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 80000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2023-03-04 22:46:00.015288: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 20000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:3\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "        dim {\n",
      "          size: 96\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.astype('float32') / 255.0\n",
    "train_images, test_images = split_train_test(data)\n",
    "\n",
    "print('train_images shape: {}'.format(train_images.shape))\n",
    "\n",
    "buffer_size       = len(train_images) # buffer size for shuffling\n",
    "global_batch_size = constants.BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync # global batch size (in our case 2gpu * BATCH_SIZE_PER_REPLICA)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(global_batch_size)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices(test_images).batch(global_batch_size)\n",
    "\n",
    "print('Distributing data...')\n",
    "train_dataset_dist = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dataset_dist  = strategy.experimental_distribute_dataset(test_dataset)\n",
    "\n",
    "del train_dataset, test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    vae       = BalleFFP(N=128, M=192, k2=3, c=3, format=ch_format)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  @tf.function\n",
    "  def Loss(inputs, outputs):\n",
    "    return tf.reduce_mean(tf.square(inputs - outputs[0])) + 0.5*(tf.reduce_mean(outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    @tf.function # compile the function to a graph for faster execution\n",
    "    def train_step(inputs, vae):\n",
    "        \"\"\"Train step function.\"\"\"\n",
    "        with tf.GradientTape() as tape: # create a tape to record operations\n",
    "            reconstructed, rateb = vae(inputs) # forward pass\n",
    "            loss          = Loss(inputs, (reconstructed, rateb)) # MSE loss (maybe put this in a function)\n",
    "            \n",
    "        gradients = tape.gradient(loss, vae.trainable_variables) # compute gradients    \n",
    "        optimizer.apply_gradients(zip(gradients, vae.trainable_variables)) # gradient descent\n",
    "        return loss # return loss for logging\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(inputs, vae):\n",
    "        outputs = vae(inputs, training=False) # forward pass\n",
    "        loss    = Loss(inputs, outputs)       \n",
    "        return loss \n",
    "    \n",
    "\n",
    "def train_step_dist(inputs, vae, strategy):\n",
    "    loss = strategy.run(train_step, args=(inputs, vae))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)\n",
    "    \n",
    "def val_step_dist(inputs, vae, strategy):\n",
    "    loss = strategy.run(val_step, args=(inputs, vae))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbc4b51dcf9406b9f7aab7558d93d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training steps: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 22:46:19.315534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-04 22:46:19.973024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-04 22:46:21.758929: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f7baeaf5330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-04 22:46:21.758982: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-03-04 22:46:21.758987: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "2023-03-04 22:46:21.765767: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-04 22:46:21.893444: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m tqdm(train_dataset_dist, \u001b[39m'\u001b[39m\u001b[39mtraining steps\u001b[39m\u001b[39m'\u001b[39m): \n\u001b[0;32m----> 5\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_step_dist(inputs, vae, strategy) \u001b[39m# type: ignore # sum losses across replicas\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     num_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# count number of batches\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m num_batches \u001b[39m# compute average loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mtrain_step_dist\u001b[0;34m(inputs, vae, strategy)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step_dist\u001b[39m(inputs, vae, strategy):\n\u001b[0;32m---> 21\u001b[0m     loss \u001b[39m=\u001b[39m strategy\u001b[39m.\u001b[39;49mrun(train_step, args\u001b[39m=\u001b[39;49m(inputs, vae))\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m strategy\u001b[39m.\u001b[39mreduce(tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mReduceOp\u001b[39m.\u001b[39mSUM, loss, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   2894\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2895\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_strategy.py:696\u001b[0m, in \u001b[0;36mMirroredExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m--> 696\u001b[0m   \u001b[39mreturn\u001b[39;00m mirrored_run\u001b[39m.\u001b[39;49mcall_for_each_replica(\n\u001b[1;32m    697\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_container_strategy(), fn, args, kwargs)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_run.py:83\u001b[0m, in \u001b[0;36mcall_for_each_replica\u001b[0;34m(strategy, fn, args, kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     wrapped \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39m_clone(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         python_function\u001b[39m=\u001b[39mfunctools\u001b[39m.\u001b[39mpartial(call_for_each_replica, strategy,\n\u001b[1;32m     81\u001b[0m                                           fn\u001b[39m.\u001b[39mpython_function))\n\u001b[1;32m     82\u001b[0m     _cfer_fn_cache[strategy][fn] \u001b[39m=\u001b[39m wrapped\n\u001b[0;32m---> 83\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapped(args, kwargs)\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m     86\u001b[0m   logging\u001b[39m.\u001b[39mlog_first_n(\n\u001b[1;32m     87\u001b[0m       logging\u001b[39m.\u001b[39mWARN, \u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m eagerly has significant \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39moverhead currently. We will be working on improving \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`run` inside a tf.function to get \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mthe best performance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m strategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/data/miniconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(constants.EPOCHS):\n",
    "    total_loss = 0.0 \n",
    "    num_batches = 0 \n",
    "    for inputs in tqdm(train_dataset_dist, 'training steps'): \n",
    "        total_loss += train_step_dist(inputs, vae, strategy) # type: ignore # sum losses across replicas\n",
    "        num_batches += 1 # count number of batches\n",
    "        \n",
    "    train_loss = total_loss / num_batches # compute average loss\n",
    "    print('Epoch {} train loss: {}'.format(epoch, train_loss))\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for inputs in tqdm(test_dataset_dist, 'validation steps'): \n",
    "        total_loss += val_step_dist(inputs, vae, strategy) # type: ignore # sum losses across replicas\n",
    "        num_batches += 1 # count number of batches\n",
    "        \n",
    "    test_loss = total_loss / num_batches # compute average loss\n",
    "    print('Epoch {} test loss: {}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "220aeacb52331f97f7780d84158167722a7cc0056b78b1c27b32a8f95243ca77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
